{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d992763-2eef-4982-bc2a-8e0cd08254ed",
   "metadata": {},
   "source": [
    "# Correctionlib to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a2b8eb-fd42-46ea-822e-7c16c60fa551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import correctionlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b53802-ed70-4830-a666-c58e826e4933",
   "metadata": {},
   "source": [
    "### Setting up some global parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de4d9ed-e4c8-44e2-973f-4b0cc7166a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "objectdict = {\n",
    "    'Electron':{\n",
    "        'basedir':'POG/EGM/',\n",
    "        'jsonfile': 'electron.json',\n",
    "        'corrections':{\n",
    "            'UL-Electron-ID-SF':  'electron_id_sf.txt'\n",
    "        },\n",
    "        'outdir':'electronsf'\n",
    "    },\n",
    "    'Muon':{\n",
    "        'basedir':'POG/MUO/',\n",
    "        'jsonfile': 'muon_Z_v2.json',\n",
    "        'corrections':{\n",
    "            'NUM_MediumID_DEN_genTracks':  'muon_id_sf.txt',\n",
    "            'NUM_TightRelIso_DEN_MediumID':'muon_iso_sf.txt'\n",
    "        },\n",
    "        'outdir':'muonsf'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac86bf-e9f4-4a82-92ff-b88d066326d5",
   "metadata": {},
   "source": [
    "#### Extracting electron scale-factors in pT-eta bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50407b0b-3c92-4822-90c2-b2450811dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function for electrons loaded.\n"
     ]
    }
   ],
   "source": [
    "def parse_electron_sf(filename, correction_name, campaign):\n",
    "    scale_factors = []\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    pt_edges = None\n",
    "    eta_edges = None\n",
    "\n",
    "    for item in json_data['corrections']:\n",
    "        #Each item is a dict\n",
    "        if item['name'] != correction_name: continue\n",
    "        \n",
    "        content = item['data']['content']\n",
    "        for obj in content:\n",
    "            ### campaign name\n",
    "            if obj['key'] not in campaign: continue\n",
    "            print('Extracting data for: '+obj['key'])\n",
    "            \n",
    "            subcontent = obj['value']['content']\n",
    "            for subobj in subcontent:\n",
    "                ### sf type\n",
    "                if subobj['key'] != 'sf':continue\n",
    "                print('Extracting data for:'+subobj['key'])\n",
    "\n",
    "                subsubcontent = subobj['value']['content']\n",
    "                for subsubobj in subsubcontent:\n",
    "                    ### Working point\n",
    "                    if subsubobj['key'] != 'Medium': continue\n",
    "                    print('Extracting data for '+subsubobj['key']+' WP')\n",
    "\n",
    "                    edges = subsubobj['value']['edges']\n",
    "                    eta_edges = edges[0]\n",
    "                    pt_edges  = edges[1]\n",
    "\n",
    "                    print('Edges extracted!')\n",
    "\n",
    "    # Now that the binning is calculated,\n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    correction = correction_set[correction_name]\n",
    "    MAX_PT = 1500\n",
    "    MIN_ETA = -2.5\n",
    "    MAX_ETA = 2.5\n",
    "    \n",
    "    #Given the pt and eta edges, loop over their midvalues.\n",
    "    for eta_low, eta_high in zip(eta_edges[:-1], eta_edges[1:]):\n",
    "        for pt_low, pt_high in zip(pt_edges[:-1], pt_edges[1:]):\n",
    "\n",
    "            if not np.isfinite(eta_low):  eta_low  = MIN_ETA if eta_low == -np.inf else MAX_ETA\n",
    "            if not np.isfinite(eta_high): eta_high = MIN_ETA if eta_high == -np.inf else MAX_ETA\n",
    "            if not np.isfinite(pt_low):     pt_low = MAX_PT if pt_low == np.inf else 0\n",
    "            if not np.isfinite(pt_high):   pt_high = MAX_PT if pt_high == np.inf else 0 \n",
    "            \n",
    "            eta = (eta_low + eta_high) / 2\n",
    "            pt  = (pt_low  + pt_high) / 2\n",
    "            era = campaign.replace('_UL', '')\n",
    "            mode = \"sf\"\n",
    "            wp = \"Medium\"\n",
    "            values = [era, 'sf', wp, eta, pt]\n",
    "            values_down = [era, 'sfdown', wp, eta, pt]\n",
    "            values_up = [era, 'sfup', wp, eta, pt]\n",
    "            sfdown = correction.evaluate(*values_down)\n",
    "            sf = correction.evaluate(*values)\n",
    "            sfup = correction.evaluate(*values_up)\n",
    "            \n",
    "            scale_factors.append({\n",
    "                'campaign': campaign,\n",
    "                'eta_low' : eta_low,\n",
    "                'eta_high': eta_high, \n",
    "                'pt_low'  : pt_low,\n",
    "                'pt_high' : pt_high,\n",
    "                'sfdown'  : sfdown,\n",
    "                'sf'      : sf,\n",
    "                'sfup'    : sfup\n",
    "            })\n",
    "\n",
    "            #print(f\"Scale factor: {sf}, sfdown: {sfdown}, sfup: {sfup}\")\n",
    "            #break ### ptbin\n",
    "        #break ### etabin\n",
    "   \n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    print('Correctionlib evaluated and dataframe created.\\n')\n",
    "    return df\n",
    "\n",
    "print('Function for electrons loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063687f6-3b75-4828-b7b7-259e7ee01b2b",
   "metadata": {},
   "source": [
    "#### Extracting muonn scale-factors in pT-eta bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04b13a07-453e-41f1-a95d-87baccd16827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function for muons loaded.\n"
     ]
    }
   ],
   "source": [
    "def parse_muon_sf(filename, correction_name, campaign):\n",
    "    scale_factors=[]\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    pt_edges = None\n",
    "    eta_edges = None\n",
    "\n",
    "    for item in json_data['corrections']:\n",
    "        #Each item is a dict\n",
    "        if item['name'] != correction_name: continue\n",
    "        print(item.keys())\n",
    "\n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    return df\n",
    "\n",
    "print('Function for muons loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24310cb-8a71-4413-89bf-a93163f28c2e",
   "metadata": {},
   "source": [
    "### Main: Iterating over the object dictionary to find scale-factors for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bebdd9a9-7e04-486b-8f04-546d5e57b468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\u001b[032mProcessing corrections for: {object}\u001b[0m\n",
      "--------------------------------------------------\n",
      "\n",
      "\u001b[033mProcessing correction: NUM_MediumID_DEN_genTracks\u001b[0m\n",
      "\n",
      "Opening file: POG/MUO/2016postVFP_UL/muon_Z_v2.json\n",
      "dict_keys(['name', 'description', 'version', 'inputs', 'output', 'data'])\n",
      "Opening file: POG/MUO/2016preVFP_UL/muon_Z_v2.json\n",
      "dict_keys(['name', 'description', 'version', 'inputs', 'output', 'data'])\n",
      "Opening file: POG/MUO/2017_UL/muon_Z_v2.json\n",
      "dict_keys(['name', 'description', 'version', 'inputs', 'output', 'data'])\n",
      "Opening file: POG/MUO/2018_UL/muon_Z_v2.json\n",
      "dict_keys(['name', 'description', 'version', 'inputs', 'output', 'data'])\n",
      "\u001b[31mDataframe empty. Skipping this.\u001b[0m\n",
      "\n",
      "\u001b[033mProcessing correction: NUM_TightRelIso_DEN_MediumID\u001b[0m\n",
      "\n",
      "Opening file: POG/MUO/2016postVFP_UL/muon_Z_v2.json\n",
      "dict_keys(['name', 'description', 'version', 'inputs', 'output', 'data'])\n",
      "Opening file: POG/MUO/2016preVFP_UL/muon_Z_v2.json\n",
      "dict_keys(['name', 'description', 'version', 'inputs', 'output', 'data'])\n",
      "Opening file: POG/MUO/2017_UL/muon_Z_v2.json\n",
      "dict_keys(['name', 'description', 'version', 'inputs', 'output', 'data'])\n",
      "Opening file: POG/MUO/2018_UL/muon_Z_v2.json\n",
      "dict_keys(['name', 'description', 'version', 'inputs', 'output', 'data'])\n",
      "\u001b[31mDataframe empty. Skipping this.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for obj, val in objectdict.items():\n",
    "    \n",
    "    if obj != 'Muon': continue ### For testing purposes\n",
    "\n",
    "    print(f'\\n'+'-'*50+'\\n\\033[032mProcessing corrections for: {object}\\033[0m\\n'+'-'*50)\n",
    "    basedir = val['basedir']\n",
    "    outdir = val['outdir']\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    campaigns = os.listdir(basedir) #list only folders, not files\n",
    "    files = []\n",
    "    for camp in campaigns:\n",
    "        if camp not in ['2018_UL', '2017_UL', '2016preVFP_UL', '2016postVFP_UL']: continue\n",
    "        filename = os.path.join(basedir, camp, val['jsonfile'])\n",
    "        if os.path.exists(filename) and filename.endswith('.json'):\n",
    "            files.append((filename, camp))\n",
    "    \n",
    "    for correction in val['corrections']:\n",
    "        print(f\"\\n\\033[033mProcessing correction: {correction}\\033[0m\\n\")\n",
    "        output_filename = val['corrections'][correction]\n",
    "        os.makedirs(val['outdir'], exist_ok=True)\n",
    "\n",
    "        data = []\n",
    "        for filename, campaign in files:\n",
    "            print(f'Opening file: {filename}')\n",
    "            if obj == 'Electron': extracted_data = parse_electron_sf(filename, correction, campaign)\n",
    "            if obj == 'Muon': extracted_data = parse_muon_sf(filename, correction, campaign)\n",
    "            data.append(extracted_data)\n",
    "            \n",
    "        data = pd.concat(data, ignore_index=True)\n",
    "        columns_to_round = ['sfdown', 'sf', 'sfup']\n",
    "        if data.empty:\n",
    "            print('\\033[31mDataframe empty. Skipping this.\\033[0m')\n",
    "            continue\n",
    "            \n",
    "        data[columns_to_round] = data[columns_to_round].round(6)\n",
    "        display(data)\n",
    "\n",
    "        outfile = os.path.join(val['outdir'], output_filename)    \n",
    "        with open(outfile, 'w') as f:\n",
    "            for index, row in data.iterrows():\n",
    "                ### Text formatting:\n",
    "                formatted_row = \"\"\n",
    "                for i, column in enumerate(data.columns):\n",
    "                    if i == 0:                       formatted_row += f\"{str(row[column]):<20}\"\n",
    "                    elif column in columns_to_round: formatted_row += f\"{str(row[column]):<12}\"\n",
    "                    else:                            formatted_row += f\"{str(row[column]):<8}\"\n",
    "                f.write(formatted_row.strip() + \"\\n\")\n",
    "\n",
    "        print(f\"Data written to {outfile}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd38b2-09e2-426b-935e-ed932dffed21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c14cd7-aaa3-4718-886d-89e0b1dc2516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8196a49-ed82-4ec9-a915-46686bbe6c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
