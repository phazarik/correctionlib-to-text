{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d992763-2eef-4982-bc2a-8e0cd08254ed",
   "metadata": {},
   "source": [
    "# Correctionlib to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a2b8eb-fd42-46ea-822e-7c16c60fa551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import correctionlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b53802-ed70-4830-a666-c58e826e4933",
   "metadata": {},
   "source": [
    "### Setting up some global parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5de4d9ed-e4c8-44e2-973f-4b0cc7166a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "objectdict = {\n",
    "    'Electron':{\n",
    "        'basedir':'POG/EGM/',\n",
    "        'jsonfile': 'electron.json',\n",
    "        'corrections':{\n",
    "            'UL-Electron-ID-SF':  'electron_id_sf.txt'\n",
    "        },\n",
    "        'outdir':'electronsf'\n",
    "    },\n",
    "    'Muon':{\n",
    "        'basedir':'POG/MUO/',\n",
    "        'jsonfile': 'muon_Z_v2.json',\n",
    "        'corrections':{\n",
    "            'NUM_MediumID_DEN_genTracks':  'muon_id_sf.txt',\n",
    "            'NUM_TightRelIso_DEN_MediumID':'muon_iso_sf.txt'\n",
    "        },\n",
    "        'outdir':'muonsf'\n",
    "    },\n",
    "    'Jet-JER':{\n",
    "        'basedir':'POG/JME/',\n",
    "        'jsonfile': 'jet_jerc.json',\n",
    "        'corrections':{\n",
    "            'Summer19UL18_V5_MC_Total_AK4PFchs':        'jet_jec.txt',\n",
    "        },\n",
    "        'outdir':'jetsf'\n",
    "    }\n",
    "}\n",
    "\n",
    "def warning(text):\n",
    "    text = '\\033[031mWarning! '+text+'\\033[0m'\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac86bf-e9f4-4a82-92ff-b88d066326d5",
   "metadata": {},
   "source": [
    "#### Extracting electron scale-factors in pT-eta bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50407b0b-3c92-4822-90c2-b2450811dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function for electrons loaded.\n"
     ]
    }
   ],
   "source": [
    "def parse_electron_sf(filename, correction_name, campaign):\n",
    "    scale_factors = []\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    pt_edges = None\n",
    "    eta_edges = None\n",
    "\n",
    "    for item in json_data['corrections']:\n",
    "        #Each item is a dict\n",
    "        if item['name'] != correction_name: continue\n",
    "        \n",
    "        content = item['data']['content']\n",
    "        for obj in content:\n",
    "            ### campaign name\n",
    "            if obj['key'] not in campaign: continue\n",
    "            print('Extracting data for: '+obj['key'])\n",
    "            \n",
    "            subcontent = obj['value']['content']\n",
    "            for subobj in subcontent:\n",
    "                ### sf type\n",
    "                if subobj['key'] != 'sf':continue\n",
    "                print('Extracting data for:'+subobj['key'])\n",
    "\n",
    "                subsubcontent = subobj['value']['content']\n",
    "                for subsubobj in subsubcontent:\n",
    "                    ### Working point\n",
    "                    if subsubobj['key'] != 'Medium': continue\n",
    "                    print('Extracting data for '+subsubobj['key']+' WP')\n",
    "\n",
    "                    edges = subsubobj['value']['edges']\n",
    "                    eta_edges = edges[0]\n",
    "                    pt_edges  = edges[1]\n",
    "\n",
    "                    print('Edges extracted!')\n",
    "\n",
    "    # Now that the binning is calculated,\n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    correction = correction_set[correction_name]\n",
    "    MAX_PT = 1500\n",
    "    MIN_ETA = -2.5\n",
    "    MAX_ETA = 2.5\n",
    "    \n",
    "    #Given the pt and eta edges, loop over their midvalues.\n",
    "    for eta_low, eta_high in zip(eta_edges[:-1], eta_edges[1:]):\n",
    "        for pt_low, pt_high in zip(pt_edges[:-1], pt_edges[1:]):\n",
    "\n",
    "            if not np.isfinite(eta_low):  eta_low  = MIN_ETA if eta_low == -np.inf else MAX_ETA\n",
    "            if not np.isfinite(eta_high): eta_high = MIN_ETA if eta_high == -np.inf else MAX_ETA\n",
    "            if not np.isfinite(pt_low):     pt_low = MAX_PT if pt_low == np.inf else 0\n",
    "            if not np.isfinite(pt_high):   pt_high = MAX_PT if pt_high == np.inf else 0 \n",
    "            \n",
    "            eta = (eta_low + eta_high) / 2\n",
    "            pt  = (pt_low  + pt_high) / 2\n",
    "            era = campaign.replace('_UL', '')\n",
    "            wp = \"Medium\"\n",
    "            values = [era, 'sf', wp, eta, pt]\n",
    "            values_down = [era, 'sfdown', wp, eta, pt]\n",
    "            values_up = [era, 'sfup', wp, eta, pt]\n",
    "            sfdown = correction.evaluate(*values_down)\n",
    "            sf = correction.evaluate(*values)\n",
    "            sfup = correction.evaluate(*values_up)\n",
    "            \n",
    "            scale_factors.append({\n",
    "                'campaign': campaign,\n",
    "                'eta_low' : eta_low,\n",
    "                'eta_high': eta_high, \n",
    "                'pt_low'  : pt_low,\n",
    "                'pt_high' : pt_high,\n",
    "                'sfdown'  : sfdown,\n",
    "                'sf'      : sf,\n",
    "                'sfup'    : sfup\n",
    "            })\n",
    "\n",
    "            #print(f\"Scale factor: {sf}, sfdown: {sfdown}, sfup: {sfup}\")\n",
    "            #break ### ptbin\n",
    "        #break ### etabin\n",
    "   \n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    print('Correctionlib evaluated and dataframe created.\\n')\n",
    "    return df\n",
    "\n",
    "print('Function for electrons loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063687f6-3b75-4828-b7b7-259e7ee01b2b",
   "metadata": {},
   "source": [
    "#### Extracting muon scale-factors in pT-eta bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04b13a07-453e-41f1-a95d-87baccd16827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function for muons loaded.\n"
     ]
    }
   ],
   "source": [
    "def parse_muon_sf(filename, correction_name, campaign):\n",
    "    scale_factors=[]\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    pt_edges = None\n",
    "    eta_edges = None\n",
    "\n",
    "    for item in json_data['corrections']:\n",
    "        #Each item is a dict\n",
    "        if item['name'] != correction_name: continue\n",
    "\n",
    "        content = item['data']['content']\n",
    "        for obj in content:\n",
    "            ### eta bins\n",
    "            if obj['key'] not in campaign: continue\n",
    "            print('Extracting data for: '+obj['key'])\n",
    "            eta_edges = obj['value']['edges']\n",
    "            subcontent = obj['value']['content']\n",
    "            for subobj in subcontent:\n",
    "                ### pt bins\n",
    "                pt_edges = subobj['edges']\n",
    "                print('Edges extracted!')\n",
    "                break ### found pt endges\n",
    "\n",
    "    # Now that the binning is calculated,\n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    correction = correction_set[correction_name]\n",
    "    MAX_PT = 1500\n",
    "    MIN_ETA = 0\n",
    "    MAX_ETA = 2.5\n",
    "    \n",
    "    #Given the pt and eta edges, loop over their midvalues.\n",
    "    for eta_low, eta_high in zip(eta_edges[:-1], eta_edges[1:]):\n",
    "        for pt_low, pt_high in zip(pt_edges[:-1], pt_edges[1:]):\n",
    "            if not np.isfinite(pt_high):   pt_high = MAX_PT if pt_high == np.inf else 0\n",
    "\n",
    "            eta = (eta_low + eta_high) / 2\n",
    "            pt  = (pt_low  + pt_high) / 2\n",
    "            era = campaign\n",
    "            values = [era, eta, pt, 'sf']\n",
    "            values_down = [era, eta, pt, 'systdown']\n",
    "            values_up = [era, eta, pt, 'systup']\n",
    "            sfdown = correction.evaluate(*values_down)\n",
    "            sf = correction.evaluate(*values)\n",
    "            sfup = correction.evaluate(*values_up)\n",
    "            \n",
    "            scale_factors.append({\n",
    "                'campaign': campaign,\n",
    "                'eta_low' : eta_low,\n",
    "                'eta_high': eta_high, \n",
    "                'pt_low'  : pt_low,\n",
    "                'pt_high' : pt_high,\n",
    "                'sfdown'  : sfdown,\n",
    "                'sf'      : sf,\n",
    "                'sfup'    : sfup\n",
    "            })\n",
    "                \n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    return df\n",
    "\n",
    "print('Function for muons loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd85c80-9f76-41b1-a8c3-dcf8c9e0fd05",
   "metadata": {},
   "source": [
    "#### Extracting JEC and JER in pT-eta bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "655391f8-0221-4e50-8739-94932354f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function for jets loaded.\n"
     ]
    }
   ],
   "source": [
    "def parse_jet_jer_sf(filename, correction_name, campaign):\n",
    "    scale_factors=[]\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    pt_edges = None\n",
    "    eta_edges = None\n",
    "\n",
    "    match_found = False\n",
    "    for item in json_data['corrections']:\n",
    "        if item['name'] != correction_name: continue\n",
    "        match_found = True\n",
    "            \n",
    "        #print(item['data'].keys())\n",
    "        #print(item['data']['input'])\n",
    "        #print(item['data']['edges'])\n",
    "        eta_edges = item['data']['edges']\n",
    "        content = item['data']['content']\n",
    "        for obj in content:\n",
    "            #print(obj.keys())\n",
    "            #print(obj['input'])\n",
    "            #print(obj['edges'])\n",
    "            pt_edges = obj['edges']\n",
    "            print('Edges extracted!')\n",
    "            break\n",
    "\n",
    "    if not match_found: warning(f'Not found: {correction_name}')\n",
    "    if eta_edges ==None or pt_edges == None: return pd.DataFrame([])\n",
    "\n",
    "    # Now that the binning is calculated,\n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    correction = correction_set[correction_name]\n",
    "    \n",
    "    #Given the pt and eta edges, loop over their midvalues.\n",
    "    for eta_low, eta_high in zip(eta_edges[:-1], eta_edges[1:]):\n",
    "        for pt_low, pt_high in zip(pt_edges[:-1], pt_edges[1:]):\n",
    "            \n",
    "            eta = (eta_low + eta_high) / 2\n",
    "            pt  = (pt_low  + pt_high) / 2\n",
    "            values = [eta, pt]\n",
    "            unc = correction.evaluate(*values)\n",
    "            sf = np.ones_like(unc)\n",
    "            \n",
    "            scale_factors.append({\n",
    "                'campaign': campaign,\n",
    "                'eta_low' : eta_low,\n",
    "                'eta_high': eta_high, \n",
    "                'pt_low'  : pt_low,\n",
    "                'pt_high' : pt_high,\n",
    "                'sfdown'  : sf-unc,\n",
    "                'sf'      : sf,\n",
    "                'sfup'    : sf+unc\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    return df\n",
    "\n",
    "print('Function for jets loaded.')      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24310cb-8a71-4413-89bf-a93163f28c2e",
   "metadata": {},
   "source": [
    "## Main: Iterating over the object dictionary to find scale-factors for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bebdd9a9-7e04-486b-8f04-546d5e57b468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\u001b[032mProcessing corrections for: {object}\u001b[0m\n",
      "--------------------------------------------------\n",
      "\n",
      "\u001b[033mProcessing correction: Summer19UL18_V5_MC_Total_AK4PFchs\u001b[0m\n",
      "\n",
      "Opening file: POG/JME/2016postVFP_UL/jet_jerc.json\n",
      "Edges extracted!\n",
      "Opening file: POG/JME/2016preVFP_UL/jet_jerc.json\n",
      "Edges extracted!\n",
      "Opening file: POG/JME/2017_UL/jet_jerc.json\n",
      "Edges extracted!\n",
      "Opening file: POG/JME/2018_UL/jet_jerc.json\n",
      "Edges extracted!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign</th>\n",
       "      <th>eta_low</th>\n",
       "      <th>eta_high</th>\n",
       "      <th>pt_low</th>\n",
       "      <th>pt_high</th>\n",
       "      <th>sfdown</th>\n",
       "      <th>sf</th>\n",
       "      <th>sfup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016postVFP_UL</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.89860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016postVFP_UL</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.90785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.09215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016postVFP_UL</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.91825</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.08175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016postVFP_UL</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.92705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.07295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016postVFP_UL</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.93400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.06600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7835</th>\n",
       "      <td>2018_UL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3837.0</td>\n",
       "      <td>4257.0</td>\n",
       "      <td>0.91160</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.08840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7836</th>\n",
       "      <td>2018_UL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>4257.0</td>\n",
       "      <td>4719.0</td>\n",
       "      <td>0.91060</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.08940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7837</th>\n",
       "      <td>2018_UL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>4719.0</td>\n",
       "      <td>5226.5</td>\n",
       "      <td>0.90965</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.09035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7838</th>\n",
       "      <td>2018_UL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5226.5</td>\n",
       "      <td>5784.0</td>\n",
       "      <td>0.90875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.09125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7839</th>\n",
       "      <td>2018_UL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5784.0</td>\n",
       "      <td>6538.0</td>\n",
       "      <td>0.90775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.09225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7840 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            campaign  eta_low  eta_high  pt_low  pt_high   sfdown   sf  \\\n",
       "0     2016postVFP_UL     -5.4      -5.0     9.0     11.0  0.89860  1.0   \n",
       "1     2016postVFP_UL     -5.4      -5.0    11.0     13.5  0.90785  1.0   \n",
       "2     2016postVFP_UL     -5.4      -5.0    13.5     16.5  0.91825  1.0   \n",
       "3     2016postVFP_UL     -5.4      -5.0    16.5     19.5  0.92705  1.0   \n",
       "4     2016postVFP_UL     -5.4      -5.0    19.5     22.5  0.93400  1.0   \n",
       "...              ...      ...       ...     ...      ...      ...  ...   \n",
       "7835         2018_UL      5.0       5.4  3837.0   4257.0  0.91160  1.0   \n",
       "7836         2018_UL      5.0       5.4  4257.0   4719.0  0.91060  1.0   \n",
       "7837         2018_UL      5.0       5.4  4719.0   5226.5  0.90965  1.0   \n",
       "7838         2018_UL      5.0       5.4  5226.5   5784.0  0.90875  1.0   \n",
       "7839         2018_UL      5.0       5.4  5784.0   6538.0  0.90775  1.0   \n",
       "\n",
       "         sfup  \n",
       "0     1.10140  \n",
       "1     1.09215  \n",
       "2     1.08175  \n",
       "3     1.07295  \n",
       "4     1.06600  \n",
       "...       ...  \n",
       "7835  1.08840  \n",
       "7836  1.08940  \n",
       "7837  1.09035  \n",
       "7838  1.09125  \n",
       "7839  1.09225  \n",
       "\n",
       "[7840 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to jetsf/jet_jec.txt\n"
     ]
    }
   ],
   "source": [
    "for obj, val in objectdict.items():\n",
    "    \n",
    "    if obj != 'Jet-JER': continue ### For testing purposes\n",
    "\n",
    "    print(f'\\n'+'-'*50+'\\n\\033[032mProcessing corrections for: {object}\\033[0m\\n'+'-'*50)\n",
    "    basedir = val['basedir']\n",
    "    outdir = val['outdir']\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    campaigns = os.listdir(basedir) #list only folders, not files\n",
    "    files = []\n",
    "    for camp in campaigns:\n",
    "        if camp not in ['2018_UL', '2017_UL', '2016preVFP_UL', '2016postVFP_UL']: continue\n",
    "        filename = os.path.join(basedir, camp, val['jsonfile'])\n",
    "        if os.path.exists(filename) and filename.endswith('.json'):\n",
    "            files.append((filename, camp))\n",
    "    \n",
    "    for correction in val['corrections']:\n",
    "        print(f\"\\n\\033[033mProcessing correction: {correction}\\033[0m\\n\")\n",
    "        output_filename = val['corrections'][correction]\n",
    "        os.makedirs(val['outdir'], exist_ok=True)\n",
    "\n",
    "        data = []\n",
    "        for filename, campaign in files:\n",
    "            print(f'Opening file: {filename}')\n",
    "            if obj == 'Electron': extracted_data = parse_electron_sf(filename, correction, campaign)\n",
    "            if obj == 'Muon':     extracted_data = parse_muon_sf(filename, correction, campaign)\n",
    "            if obj == 'Jet-JER':\n",
    "                if correction == 'Summer19UL18_V5_MC_Total_AK4PFchs':\n",
    "                    if   '2016preVFP'  in campaign: correction = 'Summer19UL16APV_V7_MC_Total_AK4PFchs'\n",
    "                    elif '2016postVFP' in campaign: correction = 'Summer19UL16_V7_MC_Total_AK4PFchs'\n",
    "                    elif '2017'        in campaign: correction = 'Summer19UL17_V5_MC_Total_AK4PFchs'\n",
    "                #print(campaign, '\\t', correction)\n",
    "                extracted_data = parse_jet_jer_sf(filename, correction, campaign)\n",
    "                correction = 'Summer19UL18_V5_MC_Total_AK4PFchs' ### return to previous so that this loop can happen again.\n",
    "            data.append(extracted_data)\n",
    "            \n",
    "        data = pd.concat(data, ignore_index=True)\n",
    "        columns_to_round = ['sfdown', 'sf', 'sfup']\n",
    "        if data.empty:\n",
    "            print('\\033[31mDataframe empty. Skipping this.\\033[0m')\n",
    "            continue\n",
    "            \n",
    "        data[columns_to_round] = data[columns_to_round].round(6)\n",
    "        display(data)\n",
    "\n",
    "        outfile = os.path.join(val['outdir'], output_filename)    \n",
    "        with open(outfile, 'w') as f:\n",
    "            for index, row in data.iterrows():\n",
    "                ### Text formatting:\n",
    "                formatted_row = \"\"\n",
    "                for i, column in enumerate(data.columns):\n",
    "                    if i == 0:                       formatted_row += f\"{str(row[column]):<20}\"\n",
    "                    elif column in columns_to_round: formatted_row += f\"{str(row[column]):<12}\"\n",
    "                    else:                            formatted_row += f\"{str(row[column]):<8}\"\n",
    "                f.write(formatted_row.strip() + \"\\n\")\n",
    "\n",
    "        print(f\"Data written to {outfile}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8196a49-ed82-4ec9-a915-46686bbe6c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03a37f-346b-49fb-b24e-90a9e8481a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
