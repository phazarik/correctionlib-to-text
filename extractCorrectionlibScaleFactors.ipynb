{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d992763-2eef-4982-bc2a-8e0cd08254ed",
   "metadata": {},
   "source": [
    "# Correctionlib to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a2b8eb-fd42-46ea-822e-7c16c60fa551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import correctionlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b53802-ed70-4830-a666-c58e826e4933",
   "metadata": {},
   "source": [
    "### Setting up some global parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de4d9ed-e4c8-44e2-973f-4b0cc7166a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "objectdict = {\n",
    "    'Electron':{\n",
    "        'basedir':'POG/EGM/',\n",
    "        'jsonfile': 'electron.json',\n",
    "        'corrections':{\n",
    "            'UL-Electron-ID-SF':  'electron_id_sf.txt'\n",
    "        },\n",
    "        'outdir':'corrections/electronsf'\n",
    "    },\n",
    "    'Muon':{\n",
    "        'basedir':'POG/MUO/',\n",
    "        'jsonfile': 'muon_Z_v2.json',\n",
    "        'corrections':{\n",
    "            'NUM_MediumID_DEN_genTracks':  'muon_id_sf.txt',\n",
    "            'NUM_TightRelIso_DEN_MediumID':'muon_iso_sf.txt'\n",
    "        },\n",
    "        'outdir':'corrections/muonsf'\n",
    "    },\n",
    "    'Jet-JEC':{\n",
    "        'basedir':'POG/JME/',\n",
    "        'jsonfile': 'jet_jerc.json',\n",
    "        'corrections':{\n",
    "            'Summer19UL18_V5_MC_Total_AK4PFchs': 'jet_jec_sf.txt',\n",
    "        },\n",
    "        'outdir':'corrections/jetsf'\n",
    "    },\n",
    "    'Jet-JER':{\n",
    "        'basedir':'POG/JME/',\n",
    "        'jsonfile': 'jet_jerc.json',\n",
    "        'corrections':{\n",
    "            'Summer19UL18_JRV2_MC_ScaleFactor_AK4PFchs': 'jet_jer_sf.txt',\n",
    "        },\n",
    "        'outdir':'corrections/jetsf'\n",
    "    },\n",
    "    'Jet-pT':{\n",
    "        'basedir':'POG/JME/',\n",
    "        'jsonfile': 'jet_jerc.json',\n",
    "        'corrections':{\n",
    "            'Summer19UL18_JRV2_MC_PtResolution_AK4PFchs': 'jet_ptres_sf.txt',\n",
    "        },\n",
    "        'outdir':'corrections/jetsf'\n",
    "    },\n",
    "    'bJet':{\n",
    "        'basedir':'POG/BTV/',\n",
    "        'jsonfile': 'btagging.json',\n",
    "        'corrections':{\n",
    "            'deepJet_mujets': 'bjet_mujets_and_incl_eff.txt',\n",
    "            'deepJet_comb': 'bjet_comb_and_incl_eff.txt'\n",
    "        },\n",
    "        'outdir':'corrections/bjeteff'\n",
    "    },\n",
    "    'PU':{\n",
    "        'basedir':'POG/LUM/',\n",
    "        'jsonfile': 'puWeights.json',\n",
    "        'corrections':{\n",
    "            'deepJet_mujets': 'pileup_wt.txt',\n",
    "        },\n",
    "        'outdir':'corrections/pileupwt'\n",
    "    }\n",
    "}\n",
    "\n",
    "def warning(text):\n",
    "    text = '\\033[031mWarning! '+text+'\\033[0m'\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac86bf-e9f4-4a82-92ff-b88d066326d5",
   "metadata": {},
   "source": [
    "#### Extracting electron scale-factors in pT-eta bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50407b0b-3c92-4822-90c2-b2450811dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function for electrons loaded.\n"
     ]
    }
   ],
   "source": [
    "def parse_electron_sf(filename, correction_name, campaign):\n",
    "    scale_factors = []\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    pt_edges = None\n",
    "    eta_edges = None\n",
    "\n",
    "    for item in json_data['corrections']:\n",
    "        #Each item is a dict\n",
    "        if item['name'] != correction_name: continue\n",
    "        \n",
    "        content = item['data']['content']\n",
    "        for obj in content:\n",
    "            ### campaign name\n",
    "            if obj['key'] not in campaign: continue\n",
    "            print('Extracting data for: '+obj['key'])\n",
    "            \n",
    "            subcontent = obj['value']['content']\n",
    "            for subobj in subcontent:\n",
    "                ### sf type\n",
    "                if subobj['key'] != 'sf':continue\n",
    "                print('Extracting data for:'+subobj['key'])\n",
    "\n",
    "                subsubcontent = subobj['value']['content']\n",
    "                for subsubobj in subsubcontent:\n",
    "                    ### Working point\n",
    "                    if subsubobj['key'] != 'Medium': continue\n",
    "                    print('Extracting data for '+subsubobj['key']+' WP')\n",
    "\n",
    "                    edges = subsubobj['value']['edges']\n",
    "                    eta_edges = edges[0]\n",
    "                    pt_edges  = edges[1]\n",
    "                    print('Edges extracted!')\n",
    "\n",
    "    # Now that the binning is calculated,\n",
    "    #print(pt_edges)\n",
    "    #print(eta_edges)\n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    correction = correction_set[correction_name]\n",
    "    MAX_PT = 1500\n",
    "    MIN_ETA = -2.5\n",
    "    MAX_ETA = 2.5\n",
    "    \n",
    "    #Given the pt and eta edges, loop over their midvalues.\n",
    "    for eta_low, eta_high in zip(eta_edges[:-1], eta_edges[1:]):\n",
    "        for pt_low, pt_high in zip(pt_edges[:-1], pt_edges[1:]):\n",
    "\n",
    "            if not np.isfinite(eta_low):  eta_low  = MIN_ETA if eta_low == -np.inf else MAX_ETA\n",
    "            if not np.isfinite(eta_high): eta_high = MIN_ETA if eta_high == -np.inf else MAX_ETA\n",
    "            if not np.isfinite(pt_low):     pt_low = MAX_PT if pt_low == np.inf else 0\n",
    "            if not np.isfinite(pt_high):   pt_high = MAX_PT if pt_high == np.inf else 0 \n",
    "            \n",
    "            eta = (eta_low + eta_high) / 2\n",
    "            pt  = (pt_low  + pt_high) / 2\n",
    "            era = campaign.replace('_UL', '')\n",
    "            wp = \"Medium\"\n",
    "            values = [era, 'sf', wp, eta, pt]\n",
    "            values_down = [era, 'sfdown', wp, eta, pt]\n",
    "            values_up = [era, 'sfup', wp, eta, pt]\n",
    "            sfdown = correction.evaluate(*values_down)\n",
    "            sf = correction.evaluate(*values)\n",
    "            sfup = correction.evaluate(*values_up)\n",
    "            \n",
    "            scale_factors.append({\n",
    "                'campaign': campaign,\n",
    "                'eta_low' : eta_low,\n",
    "                'eta_high': eta_high, \n",
    "                'pt_low'  : pt_low,\n",
    "                'pt_high' : pt_high,\n",
    "                'sfdown'  : sfdown,\n",
    "                'sf'      : sf,\n",
    "                'sfup'    : sfup\n",
    "            })\n",
    "\n",
    "            #print(f\"Scale factor: {sf}, sfdown: {sfdown}, sfup: {sfup}\")\n",
    "            #break ### ptbin\n",
    "        #break ### etabin\n",
    "   \n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    print('Correctionlib evaluated and dataframe created.\\n')\n",
    "    return df\n",
    "\n",
    "print('Function for electrons loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063687f6-3b75-4828-b7b7-259e7ee01b2b",
   "metadata": {},
   "source": [
    "#### Extracting muon scale-factors in pT-eta bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b13a07-453e-41f1-a95d-87baccd16827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function for muons loaded.\n"
     ]
    }
   ],
   "source": [
    "def parse_muon_sf(filename, correction_name, campaign):\n",
    "    scale_factors=[]\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    pt_edges = None\n",
    "    eta_edges = None\n",
    "\n",
    "    for item in json_data['corrections']:\n",
    "        #Each item is a dict\n",
    "        if item['name'] != correction_name: continue\n",
    "\n",
    "        content = item['data']['content']\n",
    "        for obj in content:\n",
    "            ### eta bins\n",
    "            if obj['key'] not in campaign: continue\n",
    "            print('Extracting data for: '+obj['key'])\n",
    "            eta_edges = obj['value']['edges']\n",
    "            subcontent = obj['value']['content']\n",
    "            for subobj in subcontent:\n",
    "                ### pt bins\n",
    "                pt_edges = subobj['edges']\n",
    "                print('Edges extracted!')\n",
    "                break ### found pt endges\n",
    "\n",
    "    # Now that the binning is calculated,\n",
    "    #print(pt_edges)\n",
    "    #print(eta_edges)\n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    correction = correction_set[correction_name]\n",
    "    MAX_PT = 1500\n",
    "    MIN_ETA = 0\n",
    "    MAX_ETA = 2.5\n",
    "    \n",
    "    #Given the pt and eta edges, loop over their midvalues.\n",
    "    for eta_low, eta_high in zip(eta_edges[:-1], eta_edges[1:]):\n",
    "        for pt_low, pt_high in zip(pt_edges[:-1], pt_edges[1:]):\n",
    "            if not np.isfinite(pt_high):   pt_high = MAX_PT if pt_high == np.inf else 0\n",
    "\n",
    "            eta = (eta_low + eta_high) / 2\n",
    "            pt  = (pt_low  + pt_high) / 2\n",
    "            era = campaign\n",
    "            values = [era, eta, pt, 'sf']\n",
    "            values_down = [era, eta, pt, 'systdown']\n",
    "            values_up = [era, eta, pt, 'systup']\n",
    "            sfdown = correction.evaluate(*values_down)\n",
    "            sf = correction.evaluate(*values)\n",
    "            sfup = correction.evaluate(*values_up)\n",
    "            \n",
    "            scale_factors.append({\n",
    "                'campaign': campaign,\n",
    "                'eta_low' : eta_low,\n",
    "                'eta_high': eta_high, \n",
    "                'pt_low'  : pt_low,\n",
    "                'pt_high' : pt_high,\n",
    "                'sfdown'  : sfdown,\n",
    "                'sf'      : sf,\n",
    "                'sfup'    : sfup\n",
    "            })\n",
    "                \n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    return df\n",
    "\n",
    "print('Function for muons loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd85c80-9f76-41b1-a8c3-dcf8c9e0fd05",
   "metadata": {},
   "source": [
    "#### Extracting JEC and JER in pT-eta bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655391f8-0221-4e50-8739-94932354f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function for jets loaded.\n"
     ]
    }
   ],
   "source": [
    "def parse_jet_jec_sf(filename, correction_name, campaign):\n",
    "    scale_factors=[]\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    pt_edges = None\n",
    "    eta_edges = None\n",
    "\n",
    "    match_found = False\n",
    "    for item in json_data['corrections']:\n",
    "        if item['name'] != correction_name: continue\n",
    "        match_found = True\n",
    "            \n",
    "        #print(item['data'].keys())\n",
    "        #print(item['data']['input'])\n",
    "        #print(item['data']['edges'])\n",
    "        eta_edges = item['data']['edges']\n",
    "        content = item['data']['content']\n",
    "        for obj in content:\n",
    "            #print(obj.keys())\n",
    "            #print(obj['input'])\n",
    "            #print(obj['edges'])\n",
    "            pt_edges = obj['edges']\n",
    "            print('Edges extracted!')\n",
    "            break\n",
    "\n",
    "    if not match_found: warning(f'Not found: {correction_name}')\n",
    "    if eta_edges ==None or pt_edges == None: return pd.DataFrame([])\n",
    "    #print(f'JEC pT edges = {pt_edges}')\n",
    "    #print(f'JEC eta edges = {eta_edges}')\n",
    "\n",
    "    # Now that the binning is calculated,\n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    correction = correction_set[correction_name]\n",
    "    \n",
    "    #Given the pt and eta edges, loop over their midvalues.\n",
    "    for eta_low, eta_high in zip(eta_edges[:-1], eta_edges[1:]):\n",
    "        for pt_low, pt_high in zip(pt_edges[:-1], pt_edges[1:]):\n",
    "            \n",
    "            eta = (eta_low + eta_high) / 2\n",
    "            pt  = (pt_low  + pt_high) / 2\n",
    "            values = [eta, pt]\n",
    "            unc = correction.evaluate(*values)\n",
    "            sf = np.ones_like(unc)\n",
    "            \n",
    "            scale_factors.append({\n",
    "                'campaign': campaign,\n",
    "                'eta_low' : eta_low,\n",
    "                'eta_high': eta_high, \n",
    "                'pt_low'  : pt_low,\n",
    "                'pt_high' : pt_high,\n",
    "                'sfdown'  : sf-unc,\n",
    "                'sf'      : sf,\n",
    "                'sfup'    : sf+unc\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    return df\n",
    "\n",
    "def parse_jet_jer_sf(filename, correction_name, campaign):\n",
    "\n",
    "    scale_factors=[]\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    pt_edges = None\n",
    "    eta_edges = None\n",
    "\n",
    "    match_found = False\n",
    "    for item in json_data['corrections']:\n",
    "        if item['name'] != correction_name: continue\n",
    "        match_found = True\n",
    "\n",
    "        #print(item['data'].keys())\n",
    "        #print(item['data']['input'])\n",
    "        #print(item['data']['edges'])\n",
    "        eta_edges = item['data']['edges']\n",
    "        print('Edges extracted!')\n",
    "\n",
    "    if not match_found: warning(f'Not found: {correction_name}')\n",
    "    #if eta_edges ==None or pt_edges == None: return pd.DataFrame([])\n",
    "    #print(f'JER eta edges = {eta_edges}')\n",
    "    \n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    correction = correction_set[correction_name]\n",
    "    for eta_low, eta_high in zip(eta_edges[:-1], eta_edges[1:]):\n",
    "        \n",
    "        eta = (eta_low + eta_high) / 2\n",
    "        values_nom = [eta, 'nom']\n",
    "        values_up  = [eta, 'up']\n",
    "        values_down = [eta, 'down']\n",
    "        \n",
    "        sf     = correction.evaluate(*values_nom)\n",
    "        sfup   = correction.evaluate(*values_up)\n",
    "        sfdown = correction.evaluate(*values_down)\n",
    "        \n",
    "        scale_factors.append({\n",
    "            'campaign': campaign,\n",
    "            'eta_low' : eta_low,\n",
    "            'eta_high': eta_high,\n",
    "            'sfdown'  : sfdown,\n",
    "            'sf'      : sf,\n",
    "            'sfup'    : sfup\n",
    "        })\n",
    "            \n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    return df\n",
    "\n",
    "def parse_jet_ptres_sf(filename, correction_name, campaign):\n",
    "\n",
    "    scale_factors=[]\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    # This is a function based correction. So pT ednges are not available.\n",
    "    # In order to write in the text file, I pciked the same pT edges as in JEC and JER\n",
    "    pt_edges = [9.0, 11.0, 13.5, 16.5, 19.5, 22.5, 26.0, 30.0, 34.5, 40.0,\n",
    "                46.0, 52.5, 60.0, 69.0, 79.0, 90.5, 105.5, 123.5, 143.0,\n",
    "                163.5, 185.0, 208.0, 232.5, 258.5, 286.0, 331.0, 396.0,\n",
    "                468.5, 549.5, 639.0, 738.0, 847.5, 968.5, 1102.0, 1249.5,\n",
    "                1412.0, 1590.5, 1787.0, 2003.0, 2241.0, 2503.0, 2790.5, 3107.0,\n",
    "                3455.0, 3837.0, 4257.0, 4719.0, 5226.5, 5784.0, 6538.0]\n",
    "    \n",
    "    eta_edges = None\n",
    "    rho_edges = None\n",
    "\n",
    "    match_found = False\n",
    "    for item in json_data['corrections']:\n",
    "        if item['name'] != correction_name: continue\n",
    "        match_found = True\n",
    "\n",
    "        eta_edges = item['data']['edges']\n",
    "        content = item['data']['content']\n",
    "        for obj in content:\n",
    "            rho_edges = obj['edges']\n",
    "            print('Edges extracted!')\n",
    "            break\n",
    "\n",
    "    #print(f'pT-res eta edges = {eta_edges}')\n",
    "    #print(f'pT-res rho edges = {rho_edges}')\n",
    "\n",
    "    if not match_found: warning(f'Not found: {correction_name}')\n",
    "\n",
    "    # Now that the binning is calculated,\n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    correction = correction_set[correction_name]\n",
    "    \n",
    "    #Given the pt and eta edges, loop over their midvalues.\n",
    "    for eta_low, eta_high in zip(eta_edges[:-1], eta_edges[1:]):\n",
    "        for pt_low, pt_high in zip(pt_edges[:-1], pt_edges[1:]):\n",
    "            for rho_low, rho_high in zip(rho_edges[:-1], rho_edges[1:]):\n",
    "            \n",
    "                eta = (eta_low + eta_high) / 2\n",
    "                pt  = (pt_low  + pt_high) / 2\n",
    "                rho  = (rho_low  + rho_high) / 2\n",
    "                values = [eta, pt, rho]\n",
    "                sf = correction.evaluate(*values)\n",
    "                \n",
    "                scale_factors.append({\n",
    "                    'campaign': campaign,\n",
    "                    'eta_low' : eta_low,\n",
    "                    'eta_high': eta_high, \n",
    "                    'pt_low'  : pt_low,\n",
    "                    'pt_high' : pt_high,\n",
    "                    'rho_low'  : rho_low,\n",
    "                    'rho_high' : rho_high,\n",
    "                    'sf'      : sf,\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    return df\n",
    "\n",
    "print('Function for jets loaded.')      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db14dd96-3c6d-4769-a059-10672ffc1abf",
   "metadata": {},
   "source": [
    "### b-tagging / mis-tagging efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711e019f-5217-4c7d-9c4c-00c072c04606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function for b-jets loaded.\n"
     ]
    }
   ],
   "source": [
    "def parse_bjet_eff(filename, correction_name, campaign):\n",
    "\n",
    "    scale_factors=[]\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    pt_edges     = [20.0, 30.0, 50.0, 70.0, 100.0, 140.0, 200.0, 300.0, 600.0, 1000.0] #Taken from the input file\n",
    "    eta_edges = [0, 2.5] #Taken from the input file\n",
    "    flavors = [0, 5, 4]\n",
    "\n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    for flav in flavors:\n",
    "        for eta_low, eta_high in zip(eta_edges[:-1], eta_edges[1:]):\n",
    "            for pt_low, pt_high in zip(pt_edges[:-1], pt_edges[1:]):\n",
    "                \n",
    "                eta = (eta_low + eta_high) / 2\n",
    "                pt  = (pt_low  + pt_high) / 2\n",
    "                wp = 'M'\n",
    "                if flav in [4, 5]: correction = correction_set[correction_name] #For b and c jets\n",
    "                else: correction = correction_set['deepJet_incl'] #for light jets\n",
    "\n",
    "                values      = ['central', 'M', flav, eta, pt]\n",
    "                values_up   = ['up', 'M', flav, eta, pt]\n",
    "                values_down = ['down', 'M', flav, eta, pt]\n",
    "                #Options: central, up, up_correlated, up_uncorrelated, down, down_correlated, down_uncorrelated\n",
    "\n",
    "                sf     = correction.evaluate(*values)\n",
    "                sfup   = correction.evaluate(*values_up)\n",
    "                sfdown = correction.evaluate(*values_down)\n",
    "                \n",
    "                scale_factors.append({\n",
    "                    'campaign': campaign,\n",
    "                    'eta_low' : eta_low,\n",
    "                    'eta_high': eta_high, \n",
    "                    'pt_low'  : pt_low,\n",
    "                    'pt_high' : pt_high,\n",
    "                    'flav'    : flav,\n",
    "                    'sfdown'  : sfdown,\n",
    "                    'sf'      : sf,\n",
    "                    'sfup'    : sfup\n",
    "                })   \n",
    "    \n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    return df\n",
    "\n",
    "print('Function for b-jets loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1794c2f-a973-43eb-9f2f-420e4eec8677",
   "metadata": {},
   "source": [
    "### Pileup weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b503b6e0-6410-475b-869d-67aeae68999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function for pileup loaded.\n"
     ]
    }
   ],
   "source": [
    "def parse_pileup_wt(filename, campaign):\n",
    "    scale_factors=[]\n",
    "    with open(filename, \"r\") as f: json_data = json.load(f)\n",
    "\n",
    "    nint_values = None\n",
    "    correction_name = None\n",
    "\n",
    "    match_found = False\n",
    "    for item in json_data['corrections']:\n",
    "        correction_name = item['name']\n",
    "        content = item['data']['content']\n",
    "        for obj in content:\n",
    "            nint_values = obj['value']['edges']\n",
    "            break\n",
    "    \n",
    "    correction_set = correctionlib.CorrectionSet.from_file(filename)\n",
    "    correction = correction_set[correction_name]\n",
    "\n",
    "    for nint in nint_values:\n",
    "        values      = [nint, 'nominal']\n",
    "        values_up   = [nint, 'up']\n",
    "        values_down = [nint, 'down']\n",
    "        sf     = correction.evaluate(*values)\n",
    "        sfup   = correction.evaluate(*values_up)\n",
    "        sfdown = correction.evaluate(*values_down)\n",
    "\n",
    "        scale_factors.append({\n",
    "            'campaign': campaign,\n",
    "            'nint'    : int(nint),\n",
    "            'sfdown'  : sfdown,\n",
    "            'sf'      : sf,\n",
    "            'sfup'    : sfup\n",
    "        })\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(scale_factors)\n",
    "    return df\n",
    "\n",
    "print('Function for pileup loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24310cb-8a71-4413-89bf-a93163f28c2e",
   "metadata": {},
   "source": [
    "## Main: Iterating over the object dictionary to find scale-factors for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bebdd9a9-7e04-486b-8f04-546d5e57b468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\u001b[032mProcessing corrections for: PU\u001b[0m\n",
      "--------------------------------------------------\n",
      "\n",
      "\u001b[033mProcessing correction: deepJet_mujets\u001b[0m\n",
      "\n",
      "Opening file: POG/LUM/2016postVFP_UL/puWeights.json for correction: deepJet_mujets\n",
      "Opening file: POG/LUM/2016preVFP_UL/puWeights.json for correction: deepJet_mujets\n",
      "Opening file: POG/LUM/2017_UL/puWeights.json for correction: deepJet_mujets\n",
      "Opening file: POG/LUM/2018_UL/puWeights.json for correction: deepJet_mujets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign</th>\n",
       "      <th>nint</th>\n",
       "      <th>sfdown</th>\n",
       "      <th>sf</th>\n",
       "      <th>sfup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016postVFP_UL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.318110</td>\n",
       "      <td>0.277740</td>\n",
       "      <td>0.244090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016postVFP_UL</td>\n",
       "      <td>1</td>\n",
       "      <td>0.425847</td>\n",
       "      <td>0.341608</td>\n",
       "      <td>0.289523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>2018_UL</td>\n",
       "      <td>98</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>2018_UL</td>\n",
       "      <td>99</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           campaign  nint    sfdown        sf      sfup\n",
       "0    2016postVFP_UL     0  0.318110  0.277740  0.244090\n",
       "1    2016postVFP_UL     1  0.425847  0.341608  0.289523\n",
       "398         2018_UL    98  1.000000  1.000000  1.000000\n",
       "399         2018_UL    99  1.000000  1.000000  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to corrections/pileupwt/pileup_wt.txt\n",
      "\n",
      "Done!\n",
      "\n",
      "CPU times: user 44.9 ms, sys: 0 ns, total: 44.9 ms\n",
      "Wall time: 491 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for obj, val in objectdict.items():\n",
    "    \n",
    "    if obj not in ['PU']: continue ### For testing purposes\n",
    "    #if obj not in ['Jet-JEC', 'Jet-JER', 'Jet-pT']: continue ### For testing purposes\n",
    "\n",
    "    print(f'\\n'+'-'*50+f'\\n\\033[032mProcessing corrections for: {obj}\\033[0m\\n'+'-'*50)\n",
    "    basedir = val['basedir']\n",
    "    outdir = val['outdir']\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    campaigns = os.listdir(basedir) #list only folders, not files\n",
    "    files = []\n",
    "    for camp in campaigns:\n",
    "        if camp not in ['2018_UL', '2017_UL', '2016preVFP_UL', '2016postVFP_UL']: continue\n",
    "        filename = os.path.join(basedir, camp, val['jsonfile'])\n",
    "        if os.path.exists(filename) and filename.endswith('.json'):\n",
    "            files.append((filename, camp))\n",
    "    \n",
    "    for correction in val['corrections']:\n",
    "        print(f\"\\n\\033[033mProcessing correction: {correction}\\033[0m\\n\")\n",
    "        output_filename = val['corrections'][correction]\n",
    "        os.makedirs(val['outdir'], exist_ok=True)\n",
    "\n",
    "        data = []\n",
    "        for filename, campaign in files:\n",
    "            print(f'Opening file: {filename} for correction: {correction}')\n",
    "            if obj == 'Electron': extracted_data = parse_electron_sf(filename, correction, campaign)\n",
    "            elif obj == 'Muon':   extracted_data = parse_muon_sf(filename, correction, campaign)\n",
    "            elif obj == 'bJet':   extracted_data = parse_bjet_eff(filename, correction, campaign)\n",
    "            elif obj == 'PU':     extracted_data = parse_pileup_wt(filename, campaign)\n",
    "            elif obj =='Jet-JEC':\n",
    "                if correction == 'Summer19UL18_V5_MC_Total_AK4PFchs':\n",
    "                    if   '2016preVFP'  in campaign: correction = 'Summer19UL16APV_V7_MC_Total_AK4PFchs'\n",
    "                    elif '2016postVFP' in campaign: correction = 'Summer19UL16_V7_MC_Total_AK4PFchs'\n",
    "                    elif '2017'        in campaign: correction = 'Summer19UL17_V5_MC_Total_AK4PFchs'\n",
    "                    extracted_data = parse_jet_jec_sf(filename, correction, campaign)\n",
    "                correction = 'Summer19UL18_V5_MC_Total_AK4PFchs' ### return to previous so that this loop can happen again.\n",
    "            elif obj == 'Jet-JER':\n",
    "                if correction == 'Summer19UL18_JRV2_MC_ScaleFactor_AK4PFchs':\n",
    "                    if   '2016preVFP'  in campaign: correction = 'Summer20UL16APV_JRV3_MC_ScaleFactor_AK4PFchs'\n",
    "                    elif '2016postVFP' in campaign: correction = 'Summer20UL16_JRV3_MC_ScaleFactor_AK4PFchs'\n",
    "                    elif '2017'        in campaign: correction = 'Summer19UL17_JRV2_MC_ScaleFactor_AK4PFchs'\n",
    "                    extracted_data = parse_jet_jer_sf(filename, correction, campaign)\n",
    "                correction = 'Summer19UL18_JRV2_MC_ScaleFactor_AK4PFchs'  ### return to previous so that this loop can happen again.\n",
    "            elif obj == 'Jet-pT':\n",
    "                if correction == 'Summer19UL18_JRV2_MC_PtResolution_AK4PFchs':\n",
    "                    if   '2016preVFP'  in campaign: correction = 'Summer20UL16APV_JRV3_MC_PtResolution_AK4PFchs'\n",
    "                    elif '2016postVFP' in campaign: correction = 'Summer20UL16_JRV3_MC_PtResolution_AK4PFchs'\n",
    "                    elif '2017'        in campaign: correction = 'Summer19UL17_JRV2_MC_PtResolution_AK4PFchs'\n",
    "                    extracted_data = parse_jet_ptres_sf(filename, correction, campaign)\n",
    "                correction = 'Summer19UL18_JRV2_MC_PtResolution_AK4PFchs' ### return to previous so that this loop can happen again.                    \n",
    "                \n",
    "            data.append(extracted_data)\n",
    "            \n",
    "        data = pd.concat(data, ignore_index=True)\n",
    "        columns_to_round = ['sfdown', 'sf', 'sfup']\n",
    "        existing_columns = [col for col in columns_to_round if col in data.columns]  ### Filter existing columns\n",
    "        if data.empty:\n",
    "            warning(f'Dataframe empty. Skipping correction: {correction}')\n",
    "            continue\n",
    "        data[existing_columns] = data[existing_columns].round(6)\n",
    "        display(pd.concat([data.head(2), data.tail(2)]))\n",
    "\n",
    "        outfile = os.path.join(val['outdir'], output_filename)    \n",
    "        with open(outfile, 'w') as f:\n",
    "            for index, row in data.iterrows():\n",
    "                ### Text formatting:\n",
    "                formatted_row = \"\"\n",
    "                for i, column in enumerate(data.columns):\n",
    "                    if i == 0:                       formatted_row += f\"{str(row[column]):<20}\"\n",
    "                    elif column in columns_to_round: formatted_row += f\"{str(row[column]):<12}\"\n",
    "                    else:                            formatted_row += f\"{str(row[column]):<8}\"\n",
    "                f.write(formatted_row.strip() + \"\\n\")\n",
    "\n",
    "        print(f\"Data written to {outfile}\")\n",
    "\n",
    "print('\\nDone!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8196a49-ed82-4ec9-a915-46686bbe6c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2341f-422f-4002-b98b-6c39445bf498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
